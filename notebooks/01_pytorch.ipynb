{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def download_mnist(path):\n",
    "    url = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "    filename = \"mnist.pkl.gz\"\n",
    "\n",
    "    if not (path / filename).exists():\n",
    "        content = requests.get(url + filename).content\n",
    "        (path / filename).open(\"wb\").write(content)\n",
    "\n",
    "    return path / filename\n",
    "\n",
    "\n",
    "data_path = Path(\"data\") if Path(\"data\").exists() else Path(\"../data\")\n",
    "path = data_path / \"downloaded\" / \"vector-mnist\"\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "datafile = download_mnist(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def read_mnist(path):\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "    return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = read_mnist(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\"\"\"Base Dataset class.\"\"\"\n",
    "from typing import Any, Callable, Dict, Sequence, Tuple, Union\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "SequenceOrTensor = Union[Sequence, torch.Tensor]\n",
    "\n",
    "\n",
    "class BaseDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Base Dataset class that simply processes data and targets through optional transforms.\n",
    "\n",
    "    Read more: https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        commonly these are torch tensors, numpy arrays, or PIL Images\n",
    "    targets\n",
    "        commonly these are torch tensors or numpy arrays\n",
    "    transform\n",
    "        function that takes a datum and returns the same\n",
    "    target_transform\n",
    "        function that takes a target and returns the same\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: SequenceOrTensor,\n",
    "        targets: SequenceOrTensor,\n",
    "        transform: Callable = None,\n",
    "        target_transform: Callable = None,\n",
    "    ) -> None:\n",
    "        if len(data) != len(targets):\n",
    "            raise ValueError(\"Data and targets must be of equal length\")\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return length of the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Return a datum and its target, after processing by transforms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (datum, target)\n",
    "        \"\"\"\n",
    "        datum, target = self.data[index], self.targets[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            datum = self.transform(datum)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return datum, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "train_ds = BaseDataset(x_train, y_train)\n",
    "\n",
    "train_ds.data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "class MNISTDataModule:\n",
    "    url = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "    filename = \"mnist.pkl.gz\"\n",
    "    \n",
    "    def __init__(self, dir, bs=32):\n",
    "        self.dir = dir\n",
    "        self.bs = bs\n",
    "        self.path = self.dir / self.filename\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if not (self.path).exists():\n",
    "            content = requests.get(self.url + self.filename).content\n",
    "            self.path.open(\"wb\").write(content)\n",
    "\n",
    "    def setup(self):\n",
    "        with gzip.open(self.path, \"rb\") as f:\n",
    "            ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "        x_train, y_train, x_valid, y_valid = map(\n",
    "            torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    "            )\n",
    "        \n",
    "        self.train_ds = BaseDataset(x_train, y_train, transform=push_to_device, target_transform=push_to_device)\n",
    "        self.valid_ds = BaseDataset(x_valid, y_valid, transform=push_to_device, target_transform=push_to_device)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_ds, batch_size=self.bs, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.valid_ds, batch_size=2 * self.bs, shuffle=False)\n",
    "\n",
    "def push_to_device(tensor):\n",
    "    print(device)\n",
    "    return tensor.to(device)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class MNISTLogistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def cross_entropy(output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    return -output[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "def configure_optimizer(model: nn.Module) -> optim.Optimizer:\n",
    "    return optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def fit(self: nn.Module, datamodule):\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup()\n",
    "\n",
    "    val_dataloader = datamodule.val_dataloader()\n",
    "    \n",
    "    self.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(self(xb), yb) for xb, yb in val_dataloader)\n",
    "\n",
    "    print(\"before start of training:\", valid_loss / len(val_dataloader))\n",
    "\n",
    "    opt = configure_optimizer(self)\n",
    "    train_dataloader = datamodule.train_dataloader()\n",
    "    for epoch in range(epochs):\n",
    "        self.train()\n",
    "        for xb, yb in train_dataloader:\n",
    "            pred = self(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_func(self(xb), yb) for xb, yb in val_dataloader)\n",
    "\n",
    "        print(epoch, valid_loss / len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "MNISTLogistic.fit = fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "digits_to_9 = list(range(10))\n",
    "data_config = {\"input_dims\": (784,), \"mapping\": {digit: str(digit) for digit in digits_to_9}}\n",
    "data_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model = MNISTLogistic()\n",
    "model.to(device)\n",
    "\n",
    "datamodule = MNISTDataModule(dir=path, bs=32)\n",
    "\n",
    "epochs = 2\n",
    "model.fit(datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'text-recognizer' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n text-recognizer ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-recognizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd46daa45be38af9b17270160ad4c0c0c96408b9434e6adafbdcc4ce6f2bf818"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
